{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535d91f7-3837-4edb-a0b7-928e34ea9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1dde89-8f90-40ad-92e3-bdbe42629256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86fb94b-7a42-4d58-b2fe-2697d4ad09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa4b8d0-b033-4ad8-af7d-9419cef9cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! apt update && apt install -y unzip zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c130f4-bf8c-46de-bec3-c4bca806eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27a34186-e70c-4df5-84ab-f5371cc15548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  phase_2_input_data.zip\n",
      "   creating: final_test_data/\n",
      "  inflating: final_test_data/final_test_data.features  \n",
      "   creating: training_data/\n",
      "  inflating: training_data/train.features  \n",
      "  inflating: training_data/train.labels  \n"
     ]
    }
   ],
   "source": [
    "! unzip phase_2_input_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41323a1e-a108-4167-adb8-e013b39fb38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5af423bcd43cc91eb1471999d5ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1cace8-4347-4741-b468-f6e186f9a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d8229d-d19b-4e24-aa54-5a7a207d4863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bce90d10a54579ad2f3df53b8dc47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 05:39:27 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='srinathmkce/indoml_100k_llama_model_16bit', speculative_config=None, tokenizer='srinathmkce/indoml_100k_llama_model_16bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=srinathmkce/indoml_100k_llama_model_16bit, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf43a2e8f6b4d7a85a243bd16fc666e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1ecee5fda3454cafac91fe1472b014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3317c480e2d45db87e43b83309bc7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df5703f9934994802cdcfbc318c44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 05:39:29 model_runner.py:1056] Starting to load model srinathmkce/indoml_100k_llama_model_16bit...\n",
      "INFO 10-20 05:39:29 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c4059bea4945cfa9065bed8fe729ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4acccb6e1f84047918ee7b4c60a186c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78ef263e7f74f88862f997a225025f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb68f8bd11747919a199119887a253b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56012ad68c0348a3b85ac454b7d40b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0764c20e8488471180b8515b88d50fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 05:39:58 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 10-20 05:39:58 gpu_executor.py:122] # GPU blocks: 12036, # CPU blocks: 2048\n",
      "INFO 10-20 05:39:58 gpu_executor.py:126] Maximum concurrency for 1024 tokens per request: 188.06x\n",
      "INFO 10-20 05:40:00 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-20 05:40:00 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-20 05:40:12 model_runner.py:1523] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "# llm = LLM(model=\"microsoft/Phi-3.5-mini-instruct\", max_model_len=1024)\n",
    "llm = LLM(model=\"srinathmkce/indoml_100k_llama_model_16bit\", max_model_len=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9558ab7-0c23-42ac-b79a-e1a67b9e0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54475fa4-2b3f-4574-8cf6-68d2d401a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "\n",
    "# outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# # Print the outputs.\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52468737-8a32-45b4-b586-dc9005e83616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184664, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_json(\"final_test_data/final_test_data.features\", lines=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "random_df = df.sample(100)\n",
    "random_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b9f5a-95dd-4b37-b58f-079e41093ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1ee719-ce7e-48b8-8e27-d575a5da2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_list = []\n",
    "for index, row in tqdm(random_df.iterrows()):\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"human\",\n",
    "            \"content\": json.dumps({\"description\": row[\"description\"], \"retailer\": row[\"retailer\"], \"price\": row[\"price\"]})\n",
    "        }\n",
    "    ]\n",
    "    outputs = llm.chat(conversation, sampling_params=sampling_params, use_tqdm=False)\n",
    "    output_list.append(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07919e7f-f56b-413d-bd8d-4e2e6fee7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(generated_text)\n",
    "        # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7d40e75-e16e-4c47-af80-496d4ee3d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"supergroup\": \"biscuits & confectionery & snacks\", \"group\": \"snacks\", \"module\": \"snacks chips crisps\", \"brand\": \"walkers crisps\"}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb7b8a-5d63-473c-9ce1-2e120824efb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa02378-e452-4623-9328-76e54040e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"human\",\n",
    "            \"content\": json.dumps({\"description\": row[\"description\"], \"retailer\": row[\"retailer\"], \"price\": row[\"price\"]})\n",
    "        }\n",
    "    ]\n",
    "    print(conversation)\n",
    "    output = llm.chat(conversation, sampling_params=sampling_params, use_tqdm=False)\n",
    "    return row['indoml_id'], output[0].outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37297af2-1160-4d26-b31a-49df11803f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_prompt(row):\n",
    "    # Use default values for missing keys\n",
    "    description = row.get(\"description\", \"unknown\")\n",
    "    retailer = row.get(\"retailer\", \"unknown\")\n",
    "    price = row.get(\"price\", \"unknown\")\n",
    "    \n",
    "    try:\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": json.dumps({\"description\": description, \"retailer\": retailer, \"price\": price})\n",
    "            }\n",
    "        ]\n",
    "        return conversation\n",
    "    except Exception as e:\n",
    "        # If any error occurs during processing, set all values to \"unknown\"\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"human\",\n",
    "                \"content\": json.dumps({\"description\": \"unknown\", \"retailer\": \"unknown\", \"price\": \"unknown\"})\n",
    "            }\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56c8004d-692e-4c0c-921b-9975b73e864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt):\n",
    "    batch_output = []\n",
    "    outputs = llm.chat(prompt, sampling_params=sampling_params, use_tqdm=False)\n",
    "    for index, output in enumerate(outputs):\n",
    "        json_str = outputs[index].outputs[0].text\n",
    "        batch_output.append(json.loads(json_str))\n",
    "    return batch_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d62ade6e-5ac4-4b89-9077-7fef12ba5377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5770.75"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0469ab3-1ea8-4da2-9e5d-3bb88a020c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "184664it [1:06:06, 46.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batch_size = 128\n",
    "batch_conversation = []\n",
    "result = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    prompt = build_prompt(row)\n",
    "    batch_conversation.append(prompt)\n",
    "    if len(batch_conversation) == batch_size:\n",
    "        output = predict(batch_conversation)\n",
    "        result += output\n",
    "        # print(len(result))\n",
    "        batch_conversation=[]\n",
    "\n",
    "# Process any remaining samples that didn't complete a full batch\n",
    "if batch_conversation:\n",
    "    output = predict(batch_conversation)\n",
    "    result += output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941201c2-c7c9-4205-a0b1-998212528cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d60a6c77-6145-463a-b85c-726e63d92a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060321d6-d4c0-4f3b-a123-9d2c6986be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"./phase_2_test_set3_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8aa1a7a-089c-47e1-b587-b45e2a5bbce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['supergroup', 'group', 'module', 'brand', 'retailer'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f87ea2a8-cb8f-42b5-b709-38d201a1f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([df[\"indoml_id\"], result_df[['supergroup', 'group', 'module', 'brand']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e7d4756-3028-4a2c-9585-88eedfe5d897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indoml_id     0\n",
       "supergroup    0\n",
       "group         0\n",
       "module        0\n",
       "brand         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e9f8740-57ab-42a0-9edd-5dd98d31b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indoml_id      int64\n",
       "supergroup    object\n",
       "group         object\n",
       "module        object\n",
       "brand         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "687c0c29-c419-4016-af6f-bab3a0ee6253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "184664it [00:07, 23112.54it/s]\n"
     ]
    }
   ],
   "source": [
    "category_list = []\n",
    "for index, row in tqdm(result_df.iterrows()):\n",
    "    # print(row)\n",
    "    category_dict = {\n",
    "        \"indoml_id\": row[\"indoml_id\"],\n",
    "        \"supergroup\": row[\"supergroup\"],\n",
    "        \"group\": row[\"group\"],\n",
    "        \"module\": row[\"module\"],\n",
    "        \"brand\": row[\"brand\"]\n",
    "    }\n",
    "    category_list.append(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "149c6954-9633-4c48-86e0-996dcae2d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to JSON file\n",
    "import json\n",
    "with open(\"test_gradientgurus4.predict\", \"w\") as fp:\n",
    "    for row in category_list:\n",
    "        fp.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf73d76-fb23-404b-9792-733f3c33b758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
