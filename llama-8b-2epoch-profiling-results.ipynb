{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0c3d8-df99-4d0d-9ebf-8a48434acd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers scikit-learn wandb bitsandbytes accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1872968f-cdc6-4612-b71e-0491c358ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import math\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
    "from transformers import AutoModelForCausalLM\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "import pickle\n",
    "import string\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e22838-4c5f-40ea-becb-b50543653d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "PRE_TRAINING_CHECKPOINT = 'step143000'\n",
    "\n",
    "MODEL_SIZE = '70m'\n",
    "MODEL_NAME = \"srinathmkce/indoml_100k_llama_updated_dataset_epoch2\"\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "logger = logging.getLogger(__name__)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661b33d6-0a05-48a7-8021-c9220959a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_random_strings(seed):\n",
    "\n",
    "    sequences = []\n",
    "    seq= []\n",
    "    alllines = ''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sequences = [\"SYS: Hello, I am the customer support bot. What can I do for you? USR: Hello robot. I ordered a pot several days ago but I can't track it. SYS: Could you verify your full name? USR: Patrick Schug SYS: Verify your order number please. USR: It's 843-58572-7002. SYS: You can track your package with your tracking number, which is AGZIM5T6KL. Are you happy about my answer? USR: All good. See you. SYS: Have a nice day! Bye.\"]\n",
    "    tseq = [\"SYS: Hello, I am the customer support bot. What can I do for you? USR: Hi. I ordered a mobile phone several days ago but I can't track it. SYS: May I have your full name? USR: James Salim. SYS: Verify your phone number please. USR: 980.322.8737 is my number. SYS: Track your order using your tracking number, 0UOKFRS1GA. Anything else? USR: No more questions. See you. SYS: Bye.\"]\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"text\": sequences,\n",
    "        }\n",
    "    )\n",
    "    test_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"text\": tseq\n",
    "        }\n",
    "    )\n",
    "    datasets = DatasetDict(\n",
    "        {\n",
    "            \"train\": dataset,\n",
    "            \"test\": test_dataset\n",
    "        }\n",
    "    )\n",
    "    datasets.set_format(\"torch\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfeadf2c-e7ff-4208-aa5e-8ead005af079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(tokenizer,dataset):\n",
    "    def encode(example: dict):\n",
    "        sequences = example[\"text\"]\n",
    "        return tokenizer(sequences,truncation=True)\n",
    "\n",
    "    return dataset.map(\n",
    "        encode,\n",
    "        batched=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d59f66a-35c7-4dce-89a0-b7879f496f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8363394d5bb413a9d986eda11bb0760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3552eb0535934cec9b2da42e7a9d47ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f3ba4ca84e433aa7bdc623b5520c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = generate_non_random_strings(seed=42)\n",
    "encoded_dataset = tokenize_string(tokenizer, dataset)\n",
    "training_dataset = encoded_dataset.remove_columns([\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator)\n",
    "test_loader = torch.utils.data.DataLoader(training_dataset[\"test\"], shuffle=True, batch_size=1, collate_fn=data_collator)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71370e6-3e56-4d2a-adf2-e278361ec9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f8baa6-3954-4265-87aa-e5c45da4476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea3b4b7-8836-4193-8ace-85a0aea16417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-02 09:16:40 1341:1341 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters that are used for training\n",
      "4540600320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-02 09:16:41 1341:1341 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-11-02 09:16:41 1341:1341 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         1.41%      10.375ms        15.06%     111.063ms     246.259us     187.598ms        35.66%     206.006ms     456.776us           451   3821524.746  \n",
      "                                              aten::bmm         1.28%       9.452ms        10.57%      77.941ms     403.839us       1.919ms         0.36%      10.634ms      55.098us           193     23795.875  \n",
      "                                              aten::mul         1.51%      11.106ms         2.85%      20.989ms      20.260us       6.306ms         1.20%      11.810ms      11.400us          1036       494.927  \n",
      "                                              aten::add         0.36%       2.640ms         1.33%       9.815ms      43.429us     844.000us         0.16%       4.447ms      19.677us           226        67.911  \n",
      "                                            aten::empty         0.66%       4.896ms         0.82%       6.026ms       5.439us       0.000us         0.00%       0.000us       0.000us          1108            --  \n",
      "                                          aten::random_         0.01%      57.000us         0.01%      57.000us      28.500us       0.000us         0.00%       0.000us       0.000us             2            --  \n",
      "                                             aten::item         0.00%      25.000us         0.00%      30.000us       0.219us       0.000us         0.00%       0.000us       0.000us           137            --  \n",
      "                              aten::_local_scalar_dense         0.00%       5.000us         0.00%       5.000us       0.036us       0.000us         0.00%       0.000us       0.000us           137            --  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.25%       1.810ms         0.29%       2.156ms       1.078ms       0.000us         0.00%       0.000us       0.000us             2            --  \n",
      "                                         aten::randperm         0.01%      72.000us         0.02%     141.000us      35.250us       0.000us         0.00%       0.000us       0.000us             4            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 737.466ms\n",
      "Self CUDA time total: 526.072ms\n",
      "\n",
      "GFLOPs during training\n",
      "3845.883459914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "  with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "      batch = batch.to(device)\n",
    "\n",
    "      inputs = {'input_ids': batch['input_ids'],'attention_mask': batch['attention_mask'],'labels': batch['labels']}\n",
    "      #print(inputs)\n",
    "      outputs = model(**inputs) # output = loss, logits, past_key_values\n",
    "      print(\"Number of model parameters that are used for training\")\n",
    "      print(sum(p.numel() for p in model.parameters()))\n",
    "      #print(outputs)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "  print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n",
    "  print(\"GFLOPs during training\") #GigaFLOPs\n",
    "  print(sum(k.flops for k in prof.key_averages())/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68e16f1-2888-49f5-b75c-eabb7bd1ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-02 09:18:27 1341:1341 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-11-02 09:18:27 1341:1341 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-11-02 09:18:27 1341:1341 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time :1.7808001041412354\n",
      "GFLOPs during testing\n",
      "1702.927932434\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "      batch = batch.to(device)\n",
    "      inputs = {'input_ids': batch['input_ids'],'attention_mask': batch['attention_mask'],'labels': batch['labels']}\n",
    "      #print(inputs)\n",
    "      outputs = model(**inputs) # output = loss, logits, past_key_values\n",
    "print(\"Inference time :\"+str(time.time()-start))\n",
    "#print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n",
    "print(\"GFLOPs during testing\") #GigaFLOPs\n",
    "print(sum(k.flops for k in prof.key_averages())/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6f5d5-fc14-4388-8ab2-7881f20bf11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
